attention_dropout: 0.15 #0.1
batch_size: 128 #64 #256
betas:
- 0.9
- 0.999
checkpoints_path: "./checkpoints/large_diverse/PDT-oracle"
clip_grad: 0.25
deterministic_torch: false
device: cuda
embedding_dim: 128
embedding_dropout: 0 #0.1
env_name: "antmaze-large-diverse-v2"
episode_len: 1000
eval_episodes: 100 #15
eval_every: 5000
eval_seed: 43 #42 or -1 to seed every run the same
group: "dt-antmaze-large-diverse-PDT-oracle"
learning_rate: 0.0002 #0.0001 #0.004
max_action: 1.0
name: "PDT-oracle"
num_heads: 2
num_layers: 3
num_workers: 4
project: "CORL"
residual_dropout: 0.15 #0.1
reward_scale: 1.0
seq_len: 10
target_returns: [1.5]
train_seed: 16 #10
update_steps: 100000
warmup_steps: 10000
weight_decay: 0.0001
log_attn_weights: 1
log_attn_every: 250
record_video: 1
run_name: "run_12"
plan_length: 1
eval_offline_every: 5000
bg_image: "./bg_images/antmaze_medium_bg.png"
eval_path_plot_every: 2500
num_plan_points: 10
plan_bar_visualisation: false
replanning_interval: 10 #80
num_eval_videos: 0
action_noise_scale: 0.4 # 0.4
use_log_distance_plans: true # true
plan_max_trajectory_ratio: 0.5 # 0.5
state_noise_scale: 0.0 # 0.0

#checkpoint_to_load: "PDT-oracle-antmaze-large-diverse-v2-54bbde96" # Run 9
#checkpoint_step_to_load: 95000
checkpoint_to_load: "PDT-oracle-antmaze-large-diverse-v2-1fa2b5f2" # Run 4
checkpoint_step_to_load: 120000
##checkpoint_to_load: "PDT-oracle-antmaze-large-diverse-v2-bc6042b1" # Run 1
#checkpoint_step_to_load: 99999