attention_dropout: 0.15 #0.15
batch_size: 512 #64 #256
betas:
- 0.9
- 0.999
checkpoints_path: "./checkpoints/block-push/PDT-oracle"
clip_grad: 0.25
deterministic_torch: false
device: cuda
embedding_dim: 48
embedding_dropout: 0 #0.1
env_name: "BlockPushMultimodal-v0"
state_dim: 16
episode_len: 350
eval_episodes: 15 #15
eval_every: 5000
eval_seed: 43 #42 or -1 to seed every run the same
group: "dt-block-push-PDT"
learning_rate: 0.0008 #0.0002
max_action: 1.0 # normally would be 0.1, but we manually normalize and unnormalize these in make_d4rl_env.py
name: "PDT-oracle"
num_heads: 2
num_layers: 2
num_workers: 4
project: "CORL"
residual_dropout: 0.15 #0.1
reward_scale: 1.0
seq_len: 20
target_returns: [0.0]
train_seed: 16 #10
update_steps: 120000 #150000
warmup_steps: 10000
weight_decay: 0.0001
log_attn_weights: 1
log_attn_every: 250
record_video: 1
run_name: "run97"
eval_offline_every: 5000
bg_image: ""
eval_path_plot_every: 2500
num_plan_points: 10
plan_bar_visualisation: false
replanning_interval: 10 #80
num_eval_videos: 1
action_noise_scale: 0
plan_max_trajectory_ratio: 1.0 # 0.5
plan_indices: [0,1,2,3,4,5] # [0,1,2,3,4,5,10,11,12,13,14,15] # Remove end effector indices or you will get action leakage
plan_disabled: false
use_two_phase_training: false # false
is_goal_conditioned: false # true
path_viz_indices: [0,1]
plans_use_actions: false
non_plan_downweighting: 0 #-2
use_timestep_embedding: False
plan_combine_observations: False
plan_sampling_method: 2 # 1: fixed-time", 2: fixed-distance", 3: log-time", 4: log-distance
plan_use_relative_states: False
goal_representation: 3 # 1: Absolute Goal, 2: Relative goal, 3: State project to Goal and Absolute Goal
disable_return_targets: True
num_eps_with_logged_attention: 0

